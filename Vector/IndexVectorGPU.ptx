//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-24817639
// Cuda compilation tools, release 10.0, V10.0.130
// Based on LLVM 3.4svn
//

.version 6.3
.target sm_30
.address_size 64

	// .globl	_Z14IndexVectorGPUIiEvPKT_S0_PS0_S3_

.visible .entry _Z14IndexVectorGPUIiEvPKT_S0_PS0_S3_(
	.param .u64 _Z14IndexVectorGPUIiEvPKT_S0_PS0_S3__param_0,
	.param .u32 _Z14IndexVectorGPUIiEvPKT_S0_PS0_S3__param_1,
	.param .u64 _Z14IndexVectorGPUIiEvPKT_S0_PS0_S3__param_2,
	.param .u64 _Z14IndexVectorGPUIiEvPKT_S0_PS0_S3__param_3
)
{
	.local .align 16 .b8 	__local_depot0[96];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<119>;
	.reg .b64 	%rd<46>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd4, [_Z14IndexVectorGPUIiEvPKT_S0_PS0_S3__param_0];
	ld.param.u32 	%r30, [_Z14IndexVectorGPUIiEvPKT_S0_PS0_S3__param_1];
	ld.param.u64 	%rd5, [_Z14IndexVectorGPUIiEvPKT_S0_PS0_S3__param_2];
	ld.param.u64 	%rd6, [_Z14IndexVectorGPUIiEvPKT_S0_PS0_S3__param_3];
	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd5;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	setp.ge.s32	%p1, %r4, %r30;
	@%p1 bra 	BB0_13;

	cvta.to.global.u64 	%rd8, %rd4;
	shl.b32 	%r33, %r4, 3;
	mul.wide.u32 	%rd9, %r33, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.u32 	%r34, [%rd10+4];
	ld.global.u32 	%r35, [%rd10+8];
	ld.global.u32 	%r36, [%rd10+12];
	ld.global.u32 	%r37, [%rd10+16];
	ld.global.u32 	%r38, [%rd10+20];
	ld.global.u32 	%r39, [%rd10+24];
	ld.global.u32 	%r40, [%rd10+28];
	ld.global.u32 	%r41, [%rd10];
	mul.lo.s32 	%r42, %r34, 3;
	add.s32 	%r43, %r42, -2;
	mul.lo.s32 	%r44, %r41, 3;
	add.s32 	%r45, %r44, -1;
	add.s32 	%r46, %r44, -2;
	st.local.v4.u32 	[%rd3], {%r46, %r45, %r44, %r43};
	mul.lo.s32 	%r47, %r35, 3;
	add.s32 	%r48, %r47, -1;
	add.s32 	%r49, %r47, -2;
	add.s32 	%r50, %r42, -1;
	st.local.v4.u32 	[%rd3+16], {%r50, %r42, %r49, %r48};
	mul.lo.s32 	%r51, %r36, 3;
	add.s32 	%r52, %r51, -1;
	add.s32 	%r53, %r51, -2;
	st.local.v4.u32 	[%rd3+32], {%r47, %r53, %r52, %r51};
	mul.lo.s32 	%r54, %r38, 3;
	add.s32 	%r55, %r54, -2;
	mul.lo.s32 	%r56, %r37, 3;
	add.s32 	%r57, %r56, -1;
	add.s32 	%r58, %r56, -2;
	st.local.v4.u32 	[%rd3+48], {%r58, %r57, %r56, %r55};
	mul.lo.s32 	%r59, %r39, 3;
	add.s32 	%r60, %r59, -1;
	add.s32 	%r61, %r59, -2;
	add.s32 	%r62, %r54, -1;
	st.local.v4.u32 	[%rd3+64], {%r62, %r54, %r61, %r60};
	mul.lo.s32 	%r63, %r40, 3;
	add.s32 	%r64, %r63, -1;
	add.s32 	%r65, %r63, -2;
	st.local.v4.u32 	[%rd3+80], {%r59, %r65, %r64, %r63};
	mul.lo.s32 	%r5, %r4, 300;
	mul.lo.s32 	%r66, %r1, %r2;
	mul.lo.s32 	%r67, %r3, 300;
	mad.lo.s32 	%r6, %r66, 300, %r67;
	mov.u32 	%r108, 0;
	mov.u32 	%r109, %r108;

BB0_2:
	setp.gt.u32	%p2, %r109, 23;
	mov.u32 	%r118, %r109;
	@%p2 bra 	BB0_12;

	add.s32 	%r9, %r108, %r5;
	mul.wide.u32 	%rd11, %r109, 4;
	add.s64 	%rd12, %rd3, %rd11;
	ld.local.u32 	%r10, [%rd12];
	mov.u32 	%r69, 24;
	sub.s32 	%r70, %r69, %r109;
	and.b32  	%r71, %r70, 3;
	setp.eq.s32	%p3, %r71, 0;
	mov.u32 	%r118, 0;
	mov.u32 	%r114, %r109;
	@%p3 bra 	BB0_9;

	setp.eq.s32	%p4, %r71, 1;
	mov.u32 	%r112, %r10;
	mov.u32 	%r113, %r109;
	@%p4 bra 	BB0_8;

	setp.eq.s32	%p5, %r71, 2;
	mov.u32 	%r110, %r10;
	mov.u32 	%r111, %r109;
	@%p5 bra 	BB0_7;

	add.s32 	%r78, %r9, %r109;
	mul.wide.u32 	%rd13, %r78, 4;
	add.s64 	%rd14, %rd2, %rd13;
	add.s64 	%rd15, %rd1, %rd13;
	st.global.u32 	[%rd14], %r10;
	st.global.u32 	[%rd15], %r10;
	add.s32 	%r111, %r109, 1;
	ld.local.u32 	%r110, [%rd12+4];

BB0_7:
	add.s32 	%r79, %r9, %r111;
	mul.wide.u32 	%rd18, %r79, 4;
	add.s64 	%rd19, %rd2, %rd18;
	add.s64 	%rd20, %rd1, %rd18;
	max.s32 	%r80, %r10, %r110;
	st.global.u32 	[%rd19], %r80;
	min.s32 	%r81, %r110, %r10;
	st.global.u32 	[%rd20], %r81;
	add.s32 	%r113, %r111, 1;
	mul.wide.u32 	%rd21, %r113, 4;
	add.s64 	%rd22, %rd3, %rd21;
	ld.local.u32 	%r112, [%rd22];

BB0_8:
	add.s32 	%r82, %r9, %r113;
	mul.wide.u32 	%rd23, %r82, 4;
	add.s64 	%rd24, %rd2, %rd23;
	add.s64 	%rd25, %rd1, %rd23;
	max.s32 	%r83, %r10, %r112;
	st.global.u32 	[%rd24], %r83;
	min.s32 	%r84, %r112, %r10;
	st.global.u32 	[%rd25], %r84;
	add.s32 	%r114, %r113, 1;
	mov.u32 	%r118, %r114;

BB0_9:
	setp.lt.u32	%p6, %r70, 4;
	@%p6 bra 	BB0_12;

	add.s32 	%r87, %r6, %r114;
	add.s32 	%r116, %r87, %r108;
	mov.u32 	%r118, %r114;

BB0_11:
	mul.wide.u32 	%rd26, %r118, 4;
	add.s64 	%rd27, %rd3, %rd26;
	mul.wide.u32 	%rd28, %r116, 4;
	add.s64 	%rd29, %rd2, %rd28;
	add.s64 	%rd30, %rd1, %rd28;
	ld.local.u32 	%r88, [%rd27];
	max.s32 	%r89, %r10, %r88;
	min.s32 	%r90, %r88, %r10;
	st.global.u32 	[%rd29], %r89;
	st.global.u32 	[%rd30], %r90;
	add.s32 	%r91, %r118, 1;
	mul.wide.u32 	%rd31, %r91, 4;
	add.s64 	%rd32, %rd3, %rd31;
	add.s32 	%r92, %r116, 1;
	mul.wide.u32 	%rd33, %r92, 4;
	add.s64 	%rd34, %rd2, %rd33;
	add.s64 	%rd35, %rd1, %rd33;
	ld.local.u32 	%r93, [%rd32];
	max.s32 	%r94, %r10, %r93;
	min.s32 	%r95, %r93, %r10;
	st.global.u32 	[%rd34], %r94;
	st.global.u32 	[%rd35], %r95;
	add.s32 	%r96, %r118, 2;
	mul.wide.u32 	%rd36, %r96, 4;
	add.s64 	%rd37, %rd3, %rd36;
	add.s32 	%r97, %r116, 2;
	mul.wide.u32 	%rd38, %r97, 4;
	add.s64 	%rd39, %rd2, %rd38;
	add.s64 	%rd40, %rd1, %rd38;
	ld.local.u32 	%r98, [%rd37];
	max.s32 	%r99, %r10, %r98;
	min.s32 	%r100, %r98, %r10;
	st.global.u32 	[%rd39], %r99;
	st.global.u32 	[%rd40], %r100;
	add.s32 	%r101, %r118, 3;
	mul.wide.u32 	%rd41, %r101, 4;
	add.s64 	%rd42, %rd3, %rd41;
	add.s32 	%r102, %r116, 3;
	mul.wide.u32 	%rd43, %r102, 4;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd45, %rd1, %rd43;
	ld.local.u32 	%r103, [%rd42];
	max.s32 	%r104, %r10, %r103;
	min.s32 	%r105, %r103, %r10;
	st.global.u32 	[%rd44], %r104;
	st.global.u32 	[%rd45], %r105;
	add.s32 	%r116, %r116, 4;
	add.s32 	%r118, %r118, 4;
	setp.lt.u32	%p7, %r118, 24;
	@%p7 bra 	BB0_11;

BB0_12:
	not.b32 	%r106, %r109;
	add.s32 	%r107, %r108, %r106;
	add.s32 	%r108, %r107, %r118;
	add.s32 	%r109, %r109, 1;
	setp.lt.u32	%p8, %r109, 24;
	@%p8 bra 	BB0_2;

BB0_13:
	ret;
}

	// .globl	_Z14IndexVectorGPUIjEvPKT_S0_PS0_S3_
.visible .entry _Z14IndexVectorGPUIjEvPKT_S0_PS0_S3_(
	.param .u64 _Z14IndexVectorGPUIjEvPKT_S0_PS0_S3__param_0,
	.param .u32 _Z14IndexVectorGPUIjEvPKT_S0_PS0_S3__param_1,
	.param .u64 _Z14IndexVectorGPUIjEvPKT_S0_PS0_S3__param_2,
	.param .u64 _Z14IndexVectorGPUIjEvPKT_S0_PS0_S3__param_3
)
{
	.local .align 16 .b8 	__local_depot1[96];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<119>;
	.reg .b64 	%rd<46>;


	mov.u64 	%SPL, __local_depot1;
	ld.param.u64 	%rd4, [_Z14IndexVectorGPUIjEvPKT_S0_PS0_S3__param_0];
	ld.param.u32 	%r30, [_Z14IndexVectorGPUIjEvPKT_S0_PS0_S3__param_1];
	ld.param.u64 	%rd5, [_Z14IndexVectorGPUIjEvPKT_S0_PS0_S3__param_2];
	ld.param.u64 	%rd6, [_Z14IndexVectorGPUIjEvPKT_S0_PS0_S3__param_3];
	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd5;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	setp.ge.u32	%p1, %r4, %r30;
	@%p1 bra 	BB1_13;

	cvta.to.global.u64 	%rd8, %rd4;
	shl.b32 	%r33, %r4, 3;
	mul.wide.u32 	%rd9, %r33, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.u32 	%r34, [%rd10+4];
	ld.global.u32 	%r35, [%rd10+8];
	ld.global.u32 	%r36, [%rd10+12];
	ld.global.u32 	%r37, [%rd10+16];
	ld.global.u32 	%r38, [%rd10+20];
	ld.global.u32 	%r39, [%rd10+24];
	ld.global.u32 	%r40, [%rd10+28];
	ld.global.u32 	%r41, [%rd10];
	mul.lo.s32 	%r42, %r34, 3;
	add.s32 	%r43, %r42, -2;
	mul.lo.s32 	%r44, %r41, 3;
	add.s32 	%r45, %r44, -1;
	add.s32 	%r46, %r44, -2;
	st.local.v4.u32 	[%rd3], {%r46, %r45, %r44, %r43};
	mul.lo.s32 	%r47, %r35, 3;
	add.s32 	%r48, %r47, -1;
	add.s32 	%r49, %r47, -2;
	add.s32 	%r50, %r42, -1;
	st.local.v4.u32 	[%rd3+16], {%r50, %r42, %r49, %r48};
	mul.lo.s32 	%r51, %r36, 3;
	add.s32 	%r52, %r51, -1;
	add.s32 	%r53, %r51, -2;
	st.local.v4.u32 	[%rd3+32], {%r47, %r53, %r52, %r51};
	mul.lo.s32 	%r54, %r38, 3;
	add.s32 	%r55, %r54, -2;
	mul.lo.s32 	%r56, %r37, 3;
	add.s32 	%r57, %r56, -1;
	add.s32 	%r58, %r56, -2;
	st.local.v4.u32 	[%rd3+48], {%r58, %r57, %r56, %r55};
	mul.lo.s32 	%r59, %r39, 3;
	add.s32 	%r60, %r59, -1;
	add.s32 	%r61, %r59, -2;
	add.s32 	%r62, %r54, -1;
	st.local.v4.u32 	[%rd3+64], {%r62, %r54, %r61, %r60};
	mul.lo.s32 	%r63, %r40, 3;
	add.s32 	%r64, %r63, -1;
	add.s32 	%r65, %r63, -2;
	st.local.v4.u32 	[%rd3+80], {%r59, %r65, %r64, %r63};
	mul.lo.s32 	%r5, %r4, 300;
	mul.lo.s32 	%r66, %r1, %r2;
	mul.lo.s32 	%r67, %r3, 300;
	mad.lo.s32 	%r6, %r66, 300, %r67;
	mov.u32 	%r108, 0;
	mov.u32 	%r109, %r108;

BB1_2:
	setp.gt.u32	%p2, %r109, 23;
	mov.u32 	%r118, %r109;
	@%p2 bra 	BB1_12;

	add.s32 	%r9, %r108, %r5;
	mul.wide.u32 	%rd11, %r109, 4;
	add.s64 	%rd12, %rd3, %rd11;
	ld.local.u32 	%r10, [%rd12];
	mov.u32 	%r69, 24;
	sub.s32 	%r70, %r69, %r109;
	and.b32  	%r71, %r70, 3;
	setp.eq.s32	%p3, %r71, 0;
	mov.u32 	%r118, 0;
	mov.u32 	%r114, %r109;
	@%p3 bra 	BB1_9;

	setp.eq.s32	%p4, %r71, 1;
	mov.u32 	%r112, %r10;
	mov.u32 	%r113, %r109;
	@%p4 bra 	BB1_8;

	setp.eq.s32	%p5, %r71, 2;
	mov.u32 	%r110, %r10;
	mov.u32 	%r111, %r109;
	@%p5 bra 	BB1_7;

	add.s32 	%r78, %r9, %r109;
	mul.wide.u32 	%rd13, %r78, 4;
	add.s64 	%rd14, %rd2, %rd13;
	add.s64 	%rd15, %rd1, %rd13;
	st.global.u32 	[%rd14], %r10;
	st.global.u32 	[%rd15], %r10;
	add.s32 	%r111, %r109, 1;
	ld.local.u32 	%r110, [%rd12+4];

BB1_7:
	add.s32 	%r79, %r9, %r111;
	mul.wide.u32 	%rd18, %r79, 4;
	add.s64 	%rd19, %rd2, %rd18;
	add.s64 	%rd20, %rd1, %rd18;
	max.u32 	%r80, %r10, %r110;
	st.global.u32 	[%rd19], %r80;
	min.u32 	%r81, %r110, %r10;
	st.global.u32 	[%rd20], %r81;
	add.s32 	%r113, %r111, 1;
	mul.wide.u32 	%rd21, %r113, 4;
	add.s64 	%rd22, %rd3, %rd21;
	ld.local.u32 	%r112, [%rd22];

BB1_8:
	add.s32 	%r82, %r9, %r113;
	mul.wide.u32 	%rd23, %r82, 4;
	add.s64 	%rd24, %rd2, %rd23;
	add.s64 	%rd25, %rd1, %rd23;
	max.u32 	%r83, %r10, %r112;
	st.global.u32 	[%rd24], %r83;
	min.u32 	%r84, %r112, %r10;
	st.global.u32 	[%rd25], %r84;
	add.s32 	%r114, %r113, 1;
	mov.u32 	%r118, %r114;

BB1_9:
	setp.lt.u32	%p6, %r70, 4;
	@%p6 bra 	BB1_12;

	add.s32 	%r87, %r6, %r114;
	add.s32 	%r116, %r87, %r108;
	mov.u32 	%r118, %r114;

BB1_11:
	mul.wide.u32 	%rd26, %r118, 4;
	add.s64 	%rd27, %rd3, %rd26;
	mul.wide.u32 	%rd28, %r116, 4;
	add.s64 	%rd29, %rd2, %rd28;
	add.s64 	%rd30, %rd1, %rd28;
	ld.local.u32 	%r88, [%rd27];
	max.u32 	%r89, %r10, %r88;
	min.u32 	%r90, %r88, %r10;
	st.global.u32 	[%rd29], %r89;
	st.global.u32 	[%rd30], %r90;
	add.s32 	%r91, %r118, 1;
	mul.wide.u32 	%rd31, %r91, 4;
	add.s64 	%rd32, %rd3, %rd31;
	add.s32 	%r92, %r116, 1;
	mul.wide.u32 	%rd33, %r92, 4;
	add.s64 	%rd34, %rd2, %rd33;
	add.s64 	%rd35, %rd1, %rd33;
	ld.local.u32 	%r93, [%rd32];
	max.u32 	%r94, %r10, %r93;
	min.u32 	%r95, %r93, %r10;
	st.global.u32 	[%rd34], %r94;
	st.global.u32 	[%rd35], %r95;
	add.s32 	%r96, %r118, 2;
	mul.wide.u32 	%rd36, %r96, 4;
	add.s64 	%rd37, %rd3, %rd36;
	add.s32 	%r97, %r116, 2;
	mul.wide.u32 	%rd38, %r97, 4;
	add.s64 	%rd39, %rd2, %rd38;
	add.s64 	%rd40, %rd1, %rd38;
	ld.local.u32 	%r98, [%rd37];
	max.u32 	%r99, %r10, %r98;
	min.u32 	%r100, %r98, %r10;
	st.global.u32 	[%rd39], %r99;
	st.global.u32 	[%rd40], %r100;
	add.s32 	%r101, %r118, 3;
	mul.wide.u32 	%rd41, %r101, 4;
	add.s64 	%rd42, %rd3, %rd41;
	add.s32 	%r102, %r116, 3;
	mul.wide.u32 	%rd43, %r102, 4;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd45, %rd1, %rd43;
	ld.local.u32 	%r103, [%rd42];
	max.u32 	%r104, %r10, %r103;
	min.u32 	%r105, %r103, %r10;
	st.global.u32 	[%rd44], %r104;
	st.global.u32 	[%rd45], %r105;
	add.s32 	%r116, %r116, 4;
	add.s32 	%r118, %r118, 4;
	setp.lt.u32	%p7, %r118, 24;
	@%p7 bra 	BB1_11;

BB1_12:
	not.b32 	%r106, %r109;
	add.s32 	%r107, %r108, %r106;
	add.s32 	%r108, %r107, %r118;
	add.s32 	%r109, %r109, 1;
	setp.lt.u32	%p8, %r109, 24;
	@%p8 bra 	BB1_2;

BB1_13:
	ret;
}

	// .globl	_Z14IndexVectorGPUIlEvPKT_S0_PS0_S3_
.visible .entry _Z14IndexVectorGPUIlEvPKT_S0_PS0_S3_(
	.param .u64 _Z14IndexVectorGPUIlEvPKT_S0_PS0_S3__param_0,
	.param .u64 _Z14IndexVectorGPUIlEvPKT_S0_PS0_S3__param_1,
	.param .u64 _Z14IndexVectorGPUIlEvPKT_S0_PS0_S3__param_2,
	.param .u64 _Z14IndexVectorGPUIlEvPKT_S0_PS0_S3__param_3
)
{
	.local .align 16 .b8 	__local_depot2[192];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<103>;


	mov.u64 	%SPL, __local_depot2;
	ld.param.u64 	%rd9, [_Z14IndexVectorGPUIlEvPKT_S0_PS0_S3__param_0];
	ld.param.u64 	%rd10, [_Z14IndexVectorGPUIlEvPKT_S0_PS0_S3__param_1];
	ld.param.u64 	%rd11, [_Z14IndexVectorGPUIlEvPKT_S0_PS0_S3__param_2];
	ld.param.u64 	%rd12, [_Z14IndexVectorGPUIlEvPKT_S0_PS0_S3__param_3];
	cvta.to.global.u64 	%rd1, %rd12;
	cvta.to.global.u64 	%rd2, %rd11;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32	%rd14, %r4;
	setp.ge.s64	%p1, %rd14, %rd10;
	@%p1 bra 	BB2_13;

	cvta.to.global.u64 	%rd15, %rd9;
	shl.b32 	%r27, %r4, 3;
	mul.wide.u32 	%rd16, %r27, 8;
	add.s64 	%rd17, %rd15, %rd16;
	ld.global.u64 	%rd18, [%rd17+8];
	ld.global.u64 	%rd19, [%rd17+16];
	ld.global.u64 	%rd20, [%rd17+24];
	ld.global.u64 	%rd21, [%rd17+32];
	ld.global.u64 	%rd22, [%rd17+40];
	ld.global.u64 	%rd23, [%rd17+48];
	ld.global.u64 	%rd24, [%rd17+56];
	ld.global.u64 	%rd25, [%rd17];
	mul.lo.s64 	%rd26, %rd25, 3;
	add.s64 	%rd27, %rd26, -1;
	add.s64 	%rd28, %rd26, -2;
	st.local.v2.u64 	[%rd3], {%rd28, %rd27};
	mul.lo.s64 	%rd29, %rd18, 3;
	add.s64 	%rd30, %rd29, -2;
	st.local.v2.u64 	[%rd3+16], {%rd26, %rd30};
	add.s64 	%rd31, %rd29, -1;
	st.local.v2.u64 	[%rd3+32], {%rd31, %rd29};
	mul.lo.s64 	%rd32, %rd19, 3;
	add.s64 	%rd33, %rd32, -1;
	add.s64 	%rd34, %rd32, -2;
	st.local.v2.u64 	[%rd3+48], {%rd34, %rd33};
	mul.lo.s64 	%rd35, %rd20, 3;
	add.s64 	%rd36, %rd35, -2;
	st.local.v2.u64 	[%rd3+64], {%rd32, %rd36};
	add.s64 	%rd37, %rd35, -1;
	st.local.v2.u64 	[%rd3+80], {%rd37, %rd35};
	mul.lo.s64 	%rd38, %rd21, 3;
	add.s64 	%rd39, %rd38, -1;
	add.s64 	%rd40, %rd38, -2;
	st.local.v2.u64 	[%rd3+96], {%rd40, %rd39};
	mul.lo.s64 	%rd41, %rd22, 3;
	add.s64 	%rd42, %rd41, -2;
	st.local.v2.u64 	[%rd3+112], {%rd38, %rd42};
	add.s64 	%rd43, %rd41, -1;
	st.local.v2.u64 	[%rd3+128], {%rd43, %rd41};
	mul.lo.s64 	%rd44, %rd23, 3;
	add.s64 	%rd45, %rd44, -1;
	add.s64 	%rd46, %rd44, -2;
	st.local.v2.u64 	[%rd3+144], {%rd46, %rd45};
	mul.lo.s64 	%rd47, %rd24, 3;
	add.s64 	%rd48, %rd47, -2;
	st.local.v2.u64 	[%rd3+160], {%rd44, %rd48};
	add.s64 	%rd49, %rd47, -1;
	st.local.v2.u64 	[%rd3+176], {%rd49, %rd47};
	mul.lo.s32 	%r5, %r4, 300;
	mul.lo.s32 	%r28, %r1, %r2;
	mul.lo.s32 	%r29, %r3, 300;
	mad.lo.s32 	%r6, %r28, 300, %r29;
	mov.u32 	%r54, 0;
	mov.u32 	%r55, %r54;

BB2_2:
	setp.gt.u32	%p2, %r55, 23;
	mov.u32 	%r62, %r55;
	@%p2 bra 	BB2_12;

	add.s32 	%r9, %r54, %r5;
	mul.wide.u32 	%rd50, %r55, 8;
	add.s64 	%rd51, %rd3, %rd50;
	ld.local.u64 	%rd4, [%rd51];
	mov.u32 	%r31, 24;
	sub.s32 	%r32, %r31, %r55;
	and.b32  	%r33, %r32, 3;
	setp.eq.s32	%p3, %r33, 0;
	mov.u32 	%r62, 0;
	mov.u32 	%r58, %r55;
	@%p3 bra 	BB2_9;

	setp.eq.s32	%p4, %r33, 1;
	mov.u64 	%rd102, %rd4;
	mov.u32 	%r57, %r55;
	@%p4 bra 	BB2_8;

	setp.eq.s32	%p5, %r33, 2;
	mov.u64 	%rd101, %rd4;
	mov.u32 	%r56, %r55;
	@%p5 bra 	BB2_7;

	add.s32 	%r40, %r9, %r55;
	mul.wide.u32 	%rd52, %r40, 8;
	add.s64 	%rd53, %rd2, %rd52;
	add.s64 	%rd54, %rd1, %rd52;
	st.global.u64 	[%rd53], %rd4;
	st.global.u64 	[%rd54], %rd4;
	add.s32 	%r56, %r55, 1;
	ld.local.u64 	%rd101, [%rd51+8];

BB2_7:
	add.s32 	%r41, %r9, %r56;
	mul.wide.u32 	%rd57, %r41, 8;
	add.s64 	%rd58, %rd2, %rd57;
	add.s64 	%rd59, %rd1, %rd57;
	max.s64 	%rd60, %rd4, %rd101;
	st.global.u64 	[%rd58], %rd60;
	min.s64 	%rd61, %rd101, %rd4;
	st.global.u64 	[%rd59], %rd61;
	add.s32 	%r57, %r56, 1;
	mul.wide.u32 	%rd62, %r57, 8;
	add.s64 	%rd63, %rd3, %rd62;
	ld.local.u64 	%rd102, [%rd63];

BB2_8:
	add.s32 	%r42, %r9, %r57;
	mul.wide.u32 	%rd64, %r42, 8;
	add.s64 	%rd65, %rd2, %rd64;
	add.s64 	%rd66, %rd1, %rd64;
	max.s64 	%rd67, %rd4, %rd102;
	st.global.u64 	[%rd65], %rd67;
	min.s64 	%rd68, %rd102, %rd4;
	st.global.u64 	[%rd66], %rd68;
	add.s32 	%r58, %r57, 1;
	mov.u32 	%r62, %r58;

BB2_9:
	setp.lt.u32	%p6, %r32, 4;
	@%p6 bra 	BB2_12;

	add.s32 	%r45, %r6, %r58;
	add.s32 	%r60, %r45, %r54;
	mov.u32 	%r62, %r58;

BB2_11:
	mul.wide.u32 	%rd69, %r62, 8;
	add.s64 	%rd70, %rd3, %rd69;
	mul.wide.u32 	%rd71, %r60, 8;
	add.s64 	%rd72, %rd2, %rd71;
	add.s64 	%rd73, %rd1, %rd71;
	ld.local.u64 	%rd74, [%rd70];
	max.s64 	%rd75, %rd4, %rd74;
	min.s64 	%rd76, %rd74, %rd4;
	st.global.u64 	[%rd72], %rd75;
	st.global.u64 	[%rd73], %rd76;
	add.s32 	%r46, %r62, 1;
	mul.wide.u32 	%rd77, %r46, 8;
	add.s64 	%rd78, %rd3, %rd77;
	add.s32 	%r47, %r60, 1;
	mul.wide.u32 	%rd79, %r47, 8;
	add.s64 	%rd80, %rd2, %rd79;
	add.s64 	%rd81, %rd1, %rd79;
	ld.local.u64 	%rd82, [%rd78];
	max.s64 	%rd83, %rd4, %rd82;
	min.s64 	%rd84, %rd82, %rd4;
	st.global.u64 	[%rd80], %rd83;
	st.global.u64 	[%rd81], %rd84;
	add.s32 	%r48, %r62, 2;
	mul.wide.u32 	%rd85, %r48, 8;
	add.s64 	%rd86, %rd3, %rd85;
	add.s32 	%r49, %r60, 2;
	mul.wide.u32 	%rd87, %r49, 8;
	add.s64 	%rd88, %rd2, %rd87;
	add.s64 	%rd89, %rd1, %rd87;
	ld.local.u64 	%rd90, [%rd86];
	max.s64 	%rd91, %rd4, %rd90;
	min.s64 	%rd92, %rd90, %rd4;
	st.global.u64 	[%rd88], %rd91;
	st.global.u64 	[%rd89], %rd92;
	add.s32 	%r50, %r62, 3;
	mul.wide.u32 	%rd93, %r50, 8;
	add.s64 	%rd94, %rd3, %rd93;
	add.s32 	%r51, %r60, 3;
	mul.wide.u32 	%rd95, %r51, 8;
	add.s64 	%rd96, %rd2, %rd95;
	add.s64 	%rd97, %rd1, %rd95;
	ld.local.u64 	%rd98, [%rd94];
	max.s64 	%rd99, %rd4, %rd98;
	min.s64 	%rd100, %rd98, %rd4;
	st.global.u64 	[%rd96], %rd99;
	st.global.u64 	[%rd97], %rd100;
	add.s32 	%r60, %r60, 4;
	add.s32 	%r62, %r62, 4;
	setp.lt.u32	%p7, %r62, 24;
	@%p7 bra 	BB2_11;

BB2_12:
	not.b32 	%r52, %r55;
	add.s32 	%r53, %r54, %r52;
	add.s32 	%r54, %r53, %r62;
	add.s32 	%r55, %r55, 1;
	setp.lt.u32	%p8, %r55, 24;
	@%p8 bra 	BB2_2;

BB2_13:
	ret;
}

	// .globl	_Z14IndexVectorGPUImEvPKT_S0_PS0_S3_
.visible .entry _Z14IndexVectorGPUImEvPKT_S0_PS0_S3_(
	.param .u64 _Z14IndexVectorGPUImEvPKT_S0_PS0_S3__param_0,
	.param .u64 _Z14IndexVectorGPUImEvPKT_S0_PS0_S3__param_1,
	.param .u64 _Z14IndexVectorGPUImEvPKT_S0_PS0_S3__param_2,
	.param .u64 _Z14IndexVectorGPUImEvPKT_S0_PS0_S3__param_3
)
{
	.local .align 16 .b8 	__local_depot3[192];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<103>;


	mov.u64 	%SPL, __local_depot3;
	ld.param.u64 	%rd9, [_Z14IndexVectorGPUImEvPKT_S0_PS0_S3__param_0];
	ld.param.u64 	%rd10, [_Z14IndexVectorGPUImEvPKT_S0_PS0_S3__param_1];
	ld.param.u64 	%rd11, [_Z14IndexVectorGPUImEvPKT_S0_PS0_S3__param_2];
	ld.param.u64 	%rd12, [_Z14IndexVectorGPUImEvPKT_S0_PS0_S3__param_3];
	cvta.to.global.u64 	%rd1, %rd12;
	cvta.to.global.u64 	%rd2, %rd11;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32	%rd14, %r4;
	setp.ge.u64	%p1, %rd14, %rd10;
	@%p1 bra 	BB3_13;

	cvta.to.global.u64 	%rd15, %rd9;
	shl.b32 	%r27, %r4, 3;
	mul.wide.u32 	%rd16, %r27, 8;
	add.s64 	%rd17, %rd15, %rd16;
	ld.global.u64 	%rd18, [%rd17+8];
	ld.global.u64 	%rd19, [%rd17+16];
	ld.global.u64 	%rd20, [%rd17+24];
	ld.global.u64 	%rd21, [%rd17+32];
	ld.global.u64 	%rd22, [%rd17+40];
	ld.global.u64 	%rd23, [%rd17+48];
	ld.global.u64 	%rd24, [%rd17+56];
	ld.global.u64 	%rd25, [%rd17];
	mul.lo.s64 	%rd26, %rd25, 3;
	add.s64 	%rd27, %rd26, -1;
	add.s64 	%rd28, %rd26, -2;
	st.local.v2.u64 	[%rd3], {%rd28, %rd27};
	mul.lo.s64 	%rd29, %rd18, 3;
	add.s64 	%rd30, %rd29, -2;
	st.local.v2.u64 	[%rd3+16], {%rd26, %rd30};
	add.s64 	%rd31, %rd29, -1;
	st.local.v2.u64 	[%rd3+32], {%rd31, %rd29};
	mul.lo.s64 	%rd32, %rd19, 3;
	add.s64 	%rd33, %rd32, -1;
	add.s64 	%rd34, %rd32, -2;
	st.local.v2.u64 	[%rd3+48], {%rd34, %rd33};
	mul.lo.s64 	%rd35, %rd20, 3;
	add.s64 	%rd36, %rd35, -2;
	st.local.v2.u64 	[%rd3+64], {%rd32, %rd36};
	add.s64 	%rd37, %rd35, -1;
	st.local.v2.u64 	[%rd3+80], {%rd37, %rd35};
	mul.lo.s64 	%rd38, %rd21, 3;
	add.s64 	%rd39, %rd38, -1;
	add.s64 	%rd40, %rd38, -2;
	st.local.v2.u64 	[%rd3+96], {%rd40, %rd39};
	mul.lo.s64 	%rd41, %rd22, 3;
	add.s64 	%rd42, %rd41, -2;
	st.local.v2.u64 	[%rd3+112], {%rd38, %rd42};
	add.s64 	%rd43, %rd41, -1;
	st.local.v2.u64 	[%rd3+128], {%rd43, %rd41};
	mul.lo.s64 	%rd44, %rd23, 3;
	add.s64 	%rd45, %rd44, -1;
	add.s64 	%rd46, %rd44, -2;
	st.local.v2.u64 	[%rd3+144], {%rd46, %rd45};
	mul.lo.s64 	%rd47, %rd24, 3;
	add.s64 	%rd48, %rd47, -2;
	st.local.v2.u64 	[%rd3+160], {%rd44, %rd48};
	add.s64 	%rd49, %rd47, -1;
	st.local.v2.u64 	[%rd3+176], {%rd49, %rd47};
	mul.lo.s32 	%r5, %r4, 300;
	mul.lo.s32 	%r28, %r1, %r2;
	mul.lo.s32 	%r29, %r3, 300;
	mad.lo.s32 	%r6, %r28, 300, %r29;
	mov.u32 	%r54, 0;
	mov.u32 	%r55, %r54;

BB3_2:
	setp.gt.u32	%p2, %r55, 23;
	mov.u32 	%r62, %r55;
	@%p2 bra 	BB3_12;

	add.s32 	%r9, %r54, %r5;
	mul.wide.u32 	%rd50, %r55, 8;
	add.s64 	%rd51, %rd3, %rd50;
	ld.local.u64 	%rd4, [%rd51];
	mov.u32 	%r31, 24;
	sub.s32 	%r32, %r31, %r55;
	and.b32  	%r33, %r32, 3;
	setp.eq.s32	%p3, %r33, 0;
	mov.u32 	%r62, 0;
	mov.u32 	%r58, %r55;
	@%p3 bra 	BB3_9;

	setp.eq.s32	%p4, %r33, 1;
	mov.u64 	%rd102, %rd4;
	mov.u32 	%r57, %r55;
	@%p4 bra 	BB3_8;

	setp.eq.s32	%p5, %r33, 2;
	mov.u64 	%rd101, %rd4;
	mov.u32 	%r56, %r55;
	@%p5 bra 	BB3_7;

	add.s32 	%r40, %r9, %r55;
	mul.wide.u32 	%rd52, %r40, 8;
	add.s64 	%rd53, %rd2, %rd52;
	add.s64 	%rd54, %rd1, %rd52;
	st.global.u64 	[%rd53], %rd4;
	st.global.u64 	[%rd54], %rd4;
	add.s32 	%r56, %r55, 1;
	ld.local.u64 	%rd101, [%rd51+8];

BB3_7:
	add.s32 	%r41, %r9, %r56;
	mul.wide.u32 	%rd57, %r41, 8;
	add.s64 	%rd58, %rd2, %rd57;
	add.s64 	%rd59, %rd1, %rd57;
	max.u64 	%rd60, %rd4, %rd101;
	st.global.u64 	[%rd58], %rd60;
	min.u64 	%rd61, %rd101, %rd4;
	st.global.u64 	[%rd59], %rd61;
	add.s32 	%r57, %r56, 1;
	mul.wide.u32 	%rd62, %r57, 8;
	add.s64 	%rd63, %rd3, %rd62;
	ld.local.u64 	%rd102, [%rd63];

BB3_8:
	add.s32 	%r42, %r9, %r57;
	mul.wide.u32 	%rd64, %r42, 8;
	add.s64 	%rd65, %rd2, %rd64;
	add.s64 	%rd66, %rd1, %rd64;
	max.u64 	%rd67, %rd4, %rd102;
	st.global.u64 	[%rd65], %rd67;
	min.u64 	%rd68, %rd102, %rd4;
	st.global.u64 	[%rd66], %rd68;
	add.s32 	%r58, %r57, 1;
	mov.u32 	%r62, %r58;

BB3_9:
	setp.lt.u32	%p6, %r32, 4;
	@%p6 bra 	BB3_12;

	add.s32 	%r45, %r6, %r58;
	add.s32 	%r60, %r45, %r54;
	mov.u32 	%r62, %r58;

BB3_11:
	mul.wide.u32 	%rd69, %r62, 8;
	add.s64 	%rd70, %rd3, %rd69;
	mul.wide.u32 	%rd71, %r60, 8;
	add.s64 	%rd72, %rd2, %rd71;
	add.s64 	%rd73, %rd1, %rd71;
	ld.local.u64 	%rd74, [%rd70];
	max.u64 	%rd75, %rd4, %rd74;
	min.u64 	%rd76, %rd74, %rd4;
	st.global.u64 	[%rd72], %rd75;
	st.global.u64 	[%rd73], %rd76;
	add.s32 	%r46, %r62, 1;
	mul.wide.u32 	%rd77, %r46, 8;
	add.s64 	%rd78, %rd3, %rd77;
	add.s32 	%r47, %r60, 1;
	mul.wide.u32 	%rd79, %r47, 8;
	add.s64 	%rd80, %rd2, %rd79;
	add.s64 	%rd81, %rd1, %rd79;
	ld.local.u64 	%rd82, [%rd78];
	max.u64 	%rd83, %rd4, %rd82;
	min.u64 	%rd84, %rd82, %rd4;
	st.global.u64 	[%rd80], %rd83;
	st.global.u64 	[%rd81], %rd84;
	add.s32 	%r48, %r62, 2;
	mul.wide.u32 	%rd85, %r48, 8;
	add.s64 	%rd86, %rd3, %rd85;
	add.s32 	%r49, %r60, 2;
	mul.wide.u32 	%rd87, %r49, 8;
	add.s64 	%rd88, %rd2, %rd87;
	add.s64 	%rd89, %rd1, %rd87;
	ld.local.u64 	%rd90, [%rd86];
	max.u64 	%rd91, %rd4, %rd90;
	min.u64 	%rd92, %rd90, %rd4;
	st.global.u64 	[%rd88], %rd91;
	st.global.u64 	[%rd89], %rd92;
	add.s32 	%r50, %r62, 3;
	mul.wide.u32 	%rd93, %r50, 8;
	add.s64 	%rd94, %rd3, %rd93;
	add.s32 	%r51, %r60, 3;
	mul.wide.u32 	%rd95, %r51, 8;
	add.s64 	%rd96, %rd2, %rd95;
	add.s64 	%rd97, %rd1, %rd95;
	ld.local.u64 	%rd98, [%rd94];
	max.u64 	%rd99, %rd4, %rd98;
	min.u64 	%rd100, %rd98, %rd4;
	st.global.u64 	[%rd96], %rd99;
	st.global.u64 	[%rd97], %rd100;
	add.s32 	%r60, %r60, 4;
	add.s32 	%r62, %r62, 4;
	setp.lt.u32	%p7, %r62, 24;
	@%p7 bra 	BB3_11;

BB3_12:
	not.b32 	%r52, %r55;
	add.s32 	%r53, %r54, %r52;
	add.s32 	%r54, %r53, %r62;
	add.s32 	%r55, %r55, 1;
	setp.lt.u32	%p8, %r55, 24;
	@%p8 bra 	BB3_2;

BB3_13:
	ret;
}

	// .globl	_Z14IndexVectorGPUIdEvPKT_S0_PS0_S3_
.visible .entry _Z14IndexVectorGPUIdEvPKT_S0_PS0_S3_(
	.param .u64 _Z14IndexVectorGPUIdEvPKT_S0_PS0_S3__param_0,
	.param .f64 _Z14IndexVectorGPUIdEvPKT_S0_PS0_S3__param_1,
	.param .u64 _Z14IndexVectorGPUIdEvPKT_S0_PS0_S3__param_2,
	.param .u64 _Z14IndexVectorGPUIdEvPKT_S0_PS0_S3__param_3
)
{
	.local .align 16 .b8 	__local_depot4[192];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .b32 	%r<63>;
	.reg .f64 	%fd<58>;
	.reg .b64 	%rd<46>;


	mov.u64 	%SPL, __local_depot4;
	ld.param.u64 	%rd4, [_Z14IndexVectorGPUIdEvPKT_S0_PS0_S3__param_0];
	ld.param.f64 	%fd6, [_Z14IndexVectorGPUIdEvPKT_S0_PS0_S3__param_1];
	ld.param.u64 	%rd5, [_Z14IndexVectorGPUIdEvPKT_S0_PS0_S3__param_2];
	ld.param.u64 	%rd6, [_Z14IndexVectorGPUIdEvPKT_S0_PS0_S3__param_3];
	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd5;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.rn.f64.s32	%fd7, %r4;
	setp.geu.f64	%p1, %fd7, %fd6;
	@%p1 bra 	BB4_13;

	cvta.to.global.u64 	%rd8, %rd4;
	shl.b32 	%r27, %r4, 3;
	mul.wide.u32 	%rd9, %r27, 8;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f64 	%fd8, [%rd10+8];
	ld.global.f64 	%fd9, [%rd10+16];
	ld.global.f64 	%fd10, [%rd10+24];
	ld.global.f64 	%fd11, [%rd10+32];
	ld.global.f64 	%fd12, [%rd10+40];
	ld.global.f64 	%fd13, [%rd10+48];
	ld.global.f64 	%fd14, [%rd10+56];
	ld.global.f64 	%fd15, [%rd10];
	mul.f64 	%fd16, %fd15, 0d4008000000000000;
	add.f64 	%fd17, %fd16, 0dBFF0000000000000;
	add.f64 	%fd18, %fd16, 0dC000000000000000;
	st.local.v2.f64 	[%rd3], {%fd18, %fd17};
	mul.f64 	%fd19, %fd8, 0d4008000000000000;
	add.f64 	%fd20, %fd19, 0dC000000000000000;
	st.local.v2.f64 	[%rd3+16], {%fd16, %fd20};
	add.f64 	%fd21, %fd19, 0dBFF0000000000000;
	st.local.v2.f64 	[%rd3+32], {%fd21, %fd19};
	mul.f64 	%fd22, %fd9, 0d4008000000000000;
	add.f64 	%fd23, %fd22, 0dBFF0000000000000;
	add.f64 	%fd24, %fd22, 0dC000000000000000;
	st.local.v2.f64 	[%rd3+48], {%fd24, %fd23};
	mul.f64 	%fd25, %fd10, 0d4008000000000000;
	add.f64 	%fd26, %fd25, 0dC000000000000000;
	st.local.v2.f64 	[%rd3+64], {%fd22, %fd26};
	add.f64 	%fd27, %fd25, 0dBFF0000000000000;
	st.local.v2.f64 	[%rd3+80], {%fd27, %fd25};
	mul.f64 	%fd28, %fd11, 0d4008000000000000;
	add.f64 	%fd29, %fd28, 0dBFF0000000000000;
	add.f64 	%fd30, %fd28, 0dC000000000000000;
	st.local.v2.f64 	[%rd3+96], {%fd30, %fd29};
	mul.f64 	%fd31, %fd12, 0d4008000000000000;
	add.f64 	%fd32, %fd31, 0dC000000000000000;
	st.local.v2.f64 	[%rd3+112], {%fd28, %fd32};
	add.f64 	%fd33, %fd31, 0dBFF0000000000000;
	st.local.v2.f64 	[%rd3+128], {%fd33, %fd31};
	mul.f64 	%fd34, %fd13, 0d4008000000000000;
	add.f64 	%fd35, %fd34, 0dBFF0000000000000;
	add.f64 	%fd36, %fd34, 0dC000000000000000;
	st.local.v2.f64 	[%rd3+144], {%fd36, %fd35};
	mul.f64 	%fd37, %fd14, 0d4008000000000000;
	add.f64 	%fd38, %fd37, 0dC000000000000000;
	st.local.v2.f64 	[%rd3+160], {%fd34, %fd38};
	add.f64 	%fd39, %fd37, 0dBFF0000000000000;
	st.local.v2.f64 	[%rd3+176], {%fd39, %fd37};
	mul.lo.s32 	%r5, %r4, 300;
	mul.lo.s32 	%r28, %r1, %r2;
	mul.lo.s32 	%r29, %r3, 300;
	mad.lo.s32 	%r6, %r28, 300, %r29;
	mov.u32 	%r54, 0;
	mov.u32 	%r55, %r54;

BB4_2:
	setp.gt.u32	%p2, %r55, 23;
	mov.u32 	%r62, %r55;
	@%p2 bra 	BB4_12;

	add.s32 	%r9, %r54, %r5;
	mul.wide.u32 	%rd11, %r55, 8;
	add.s64 	%rd12, %rd3, %rd11;
	ld.local.f64 	%fd1, [%rd12];
	mov.u32 	%r31, 24;
	sub.s32 	%r32, %r31, %r55;
	and.b32  	%r33, %r32, 3;
	setp.eq.s32	%p3, %r33, 0;
	mov.u32 	%r62, 0;
	mov.u32 	%r58, %r55;
	@%p3 bra 	BB4_9;

	setp.eq.s32	%p4, %r33, 1;
	mov.f64 	%fd57, %fd1;
	mov.u32 	%r57, %r55;
	@%p4 bra 	BB4_8;

	setp.eq.s32	%p5, %r33, 2;
	mov.f64 	%fd56, %fd1;
	mov.u32 	%r56, %r55;
	@%p5 bra 	BB4_7;

	add.s32 	%r40, %r9, %r55;
	mul.wide.u32 	%rd13, %r40, 8;
	add.s64 	%rd14, %rd2, %rd13;
	add.s64 	%rd15, %rd1, %rd13;
	st.global.f64 	[%rd14], %fd1;
	st.global.f64 	[%rd15], %fd1;
	add.s32 	%r56, %r55, 1;
	ld.local.f64 	%fd56, [%rd12+8];

BB4_7:
	add.s32 	%r41, %r9, %r56;
	mul.wide.u32 	%rd18, %r41, 8;
	add.s64 	%rd19, %rd2, %rd18;
	add.s64 	%rd20, %rd1, %rd18;
	setp.ltu.f64	%p6, %fd56, %fd1;
	selp.f64	%fd40, %fd1, %fd56, %p6;
	selp.f64	%fd41, %fd56, %fd1, %p6;
	st.global.f64 	[%rd19], %fd40;
	st.global.f64 	[%rd20], %fd41;
	add.s32 	%r57, %r56, 1;
	mul.wide.u32 	%rd21, %r57, 8;
	add.s64 	%rd22, %rd3, %rd21;
	ld.local.f64 	%fd57, [%rd22];

BB4_8:
	add.s32 	%r42, %r9, %r57;
	mul.wide.u32 	%rd23, %r42, 8;
	add.s64 	%rd24, %rd2, %rd23;
	add.s64 	%rd25, %rd1, %rd23;
	setp.ltu.f64	%p7, %fd57, %fd1;
	selp.f64	%fd42, %fd1, %fd57, %p7;
	selp.f64	%fd43, %fd57, %fd1, %p7;
	st.global.f64 	[%rd24], %fd42;
	st.global.f64 	[%rd25], %fd43;
	add.s32 	%r58, %r57, 1;
	mov.u32 	%r62, %r58;

BB4_9:
	setp.lt.u32	%p8, %r32, 4;
	@%p8 bra 	BB4_12;

	add.s32 	%r45, %r6, %r58;
	add.s32 	%r60, %r45, %r54;
	mov.u32 	%r62, %r58;

BB4_11:
	mul.wide.u32 	%rd26, %r62, 8;
	add.s64 	%rd27, %rd3, %rd26;
	ld.local.f64 	%fd44, [%rd27];
	setp.ltu.f64	%p9, %fd44, %fd1;
	mul.wide.u32 	%rd28, %r60, 8;
	add.s64 	%rd29, %rd2, %rd28;
	add.s64 	%rd30, %rd1, %rd28;
	selp.f64	%fd45, %fd1, %fd44, %p9;
	selp.f64	%fd46, %fd44, %fd1, %p9;
	st.global.f64 	[%rd29], %fd45;
	st.global.f64 	[%rd30], %fd46;
	add.s32 	%r46, %r62, 1;
	mul.wide.u32 	%rd31, %r46, 8;
	add.s64 	%rd32, %rd3, %rd31;
	ld.local.f64 	%fd47, [%rd32];
	setp.ltu.f64	%p10, %fd47, %fd1;
	add.s32 	%r47, %r60, 1;
	mul.wide.u32 	%rd33, %r47, 8;
	add.s64 	%rd34, %rd2, %rd33;
	add.s64 	%rd35, %rd1, %rd33;
	selp.f64	%fd48, %fd1, %fd47, %p10;
	selp.f64	%fd49, %fd47, %fd1, %p10;
	st.global.f64 	[%rd34], %fd48;
	st.global.f64 	[%rd35], %fd49;
	add.s32 	%r48, %r62, 2;
	mul.wide.u32 	%rd36, %r48, 8;
	add.s64 	%rd37, %rd3, %rd36;
	ld.local.f64 	%fd50, [%rd37];
	setp.ltu.f64	%p11, %fd50, %fd1;
	add.s32 	%r49, %r60, 2;
	mul.wide.u32 	%rd38, %r49, 8;
	add.s64 	%rd39, %rd2, %rd38;
	add.s64 	%rd40, %rd1, %rd38;
	selp.f64	%fd51, %fd1, %fd50, %p11;
	selp.f64	%fd52, %fd50, %fd1, %p11;
	st.global.f64 	[%rd39], %fd51;
	st.global.f64 	[%rd40], %fd52;
	add.s32 	%r50, %r62, 3;
	mul.wide.u32 	%rd41, %r50, 8;
	add.s64 	%rd42, %rd3, %rd41;
	ld.local.f64 	%fd53, [%rd42];
	setp.ltu.f64	%p12, %fd53, %fd1;
	add.s32 	%r51, %r60, 3;
	mul.wide.u32 	%rd43, %r51, 8;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd45, %rd1, %rd43;
	selp.f64	%fd54, %fd1, %fd53, %p12;
	selp.f64	%fd55, %fd53, %fd1, %p12;
	st.global.f64 	[%rd44], %fd54;
	st.global.f64 	[%rd45], %fd55;
	add.s32 	%r60, %r60, 4;
	add.s32 	%r62, %r62, 4;
	setp.lt.u32	%p13, %r62, 24;
	@%p13 bra 	BB4_11;

BB4_12:
	not.b32 	%r52, %r55;
	add.s32 	%r53, %r54, %r52;
	add.s32 	%r54, %r53, %r62;
	add.s32 	%r55, %r55, 1;
	setp.lt.u32	%p14, %r55, 24;
	@%p14 bra 	BB4_2;

BB4_13:
	ret;
}


